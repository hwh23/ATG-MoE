defaults: 
  - arp_plus # import arp_plus.yaml to root level
  - _self_

py_module: autoregressive_policy_plus

model:
  # weights: /opt/data/private/arp/arp/assembly_skills_learning/outputs/moe_2GPU_transform_augmentationTrue/lr7.5e-06/Emb_moeTrue_Transformer_moeFalse_task_rod_sleeve_assembly/2025_09_26_32BS_2epochs/2025-09-26_00-53/model_4500.pth
  weights: /opt/data/private/arp/arp/assembly_skills_learning/outputs/moe_2GPU_transform_augmentationTrue/lr7.5e-06/Emb_moeTrue_Transformer_moeFalse_task_sleeve_fixturing/2025_09_19_32BS_2epochs/2025-09-19_15-29/model_4500.pth
  # weights: /opt/data/private/arp/arp/assembly_skills_learning/outputs/moe_2GPU_transform_augmentationTrue/lr7.5e-06/Emb_moeTrue_Transformer_moeFalse_proprio_dim8/2025_08_23_32BS_8epochs/2025-08-23_21-29/model_1000.pth

train.num_gpus: 1
# ================================================ #

hydra:
  job:
    name: tcp_moe
    chdir: false
  run:
    dir: outputs/${hydra.job.name}/lr${model.hp.lr}/Emb_moe${model.hp.is_emb_moe}_Transformer_moe${model.hp.is_transformer_moe}/${now:%Y-%m-%d_%H-%M} 
    
output_dir: ${hydra:run.dir} 

wandb: 
  project: arp_moe # before null
  use_wandb: false
  use_proxy: true
  proxy_settings:
    http_proxy: "http://10.81.36.233:7890"
    https_proxy: "http://10.81.36.233:7890"
    wandb_init_timeout: "600"
    wandb_http_timeout: "300"

swanlab:
  project: ATG_MoE
  use_swanlab: false

tcp:
  host: 127.0.0.1
  port: 12500
  max_queue_size: 10
  queue_time_out: 0.01 # seconds
  max_retries: null # retry forever
  retry_interval: 3.0 # seconds
  max_client_num: 5

