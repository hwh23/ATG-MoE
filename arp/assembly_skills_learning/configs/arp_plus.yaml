py_module: autoregressive_policy_plus

model:
  weights: /opt/data/private/arp/arp/assembly_skills_learning/outputs/moe_2GPUs/lr2.5e-05/moeTrue/2025-06-26_22-18/model_50000.pth 

  hp:
    # rot_z_weight_factor: 1
    # rot_z_weight_max: 2
    is_moe: true # true
    moe_weight: 1 #2 
    moe:
      multiple_gate: True # false
      num_experts: 4 #16 #8
      k: 1 #2 #4 #可以留一个num_moe_layers的接口
      num_shared_experts: 0 # not available yet
    add_corr: true
    add_depth: true
    add_lang: true
    add_pixel_loc: true
    add_proprio: true
    attn_dim: 512
    attn_dim_head: 64
    attn_dropout: 0.1
    attn_heads: 8
    depth: 8
    feat_dim: 220 # 72*3 + 4
    im_channels: 64
    point_augment_noise: 0.05
    img_feat_dim: 3
    img_patch_size: 14
    img_size: 224
    lang_dim: 512
    lang_len: 77
    norm_corr: true
    pe_fix: true
    proprio_dim: 3 # 4 # 18
    mvt_cameras: ['top', 'left', 'front']
    stage2_zoom_scale: 4
    stage2_waypoint_label_noise: 0.05
    rotation_aug: #null
      - [-2, -1, 0, -1, -2]
      - [0.1, 0.2, 0.4, 0.2, 0.1]
    use_xformers: true

    gt_hm_sigma: 1.5
    move_pc_in_bound: true
    place_with_mean: false

    amp: True
    bnb: True

    # lr should be thought on per sample basis
    # effective lr is multiplied by bs * num_devices
    lr: 2.5e-5 # 1e-4 #before 1.25e-5 # 1e-4 # 在train的时候cfg.model.hp.lr *= (cfg.train.num_gpus * cfg.train.bs),少GPU应对应增大lr
    warmup_steps: 2000
    optimizer_type: lamb
    lr_cos_dec: true
    add_rgc_loss: true
    transform_augmentation: true
    transform_augmentation_xyz: [0.125, 0.125, 0.125]
    transform_augmentation_rpy: [0.0, 0.0, 45.0]
    lambda_weight_l2: 1e-4 # 1e-6
    num_rotation_classes: 72

    cos_dec_max_step: -1 # will be override during training

    render_with_cpp: true

    # arp_cfg: 
    #   TODO: true


env:
  tasks: piston_sleeve_installation # stack_cups, before all
  cameras: ["front", "left_shoulder", "overhead"]
  scene_bounds: [-0.3, -0.5, 0.6, 0.7, 0.5, 1.6] # [x_min, y_min, z_min, x_max, y_max, z_max] - the metric volume to be voxelized
  image_size: 512
  time_in_state: false
  voxel_size: 100
  episode_length: 25
  rotation_resolution: 5
  origin_style_state: true

train:
  bs: 24 #48 #before 96 # 48
  demo_folder: ./data/train
  epochs: 10 # 16 # before 100 # 16
  num_gpus: 2 # 1 # before 4
  num_workers: 2 # before 8, need larger value
  num_transitions_per_epoch: 160000 # 80000 #before 160000 # iters=num_transitions_per_epoch*epochs/bs/world_size
  disp_freq: 100
  cached_dataset_path: null
  save_freq: 5000 # before 10000
  eval_mode: false
  k2k_sample_ratios: 
    piston_sleeve_installation: 1.0

eval:
  datafolder: ./data/test
  episode_num: 25
  start_episode: 0
  headless: true
  save_video: false
  device: 0


# ================================================ #

hydra:
  job:
    name: moe_2GPUs # test # default # arp_plus # eval.arp_plus
    # name: arp_plus_moe_1gate # default # arp_plus # eval.arp_plus
    chdir: false
  run:
    # dir: outputs/${hydra.job.name}/lr${model.hp.lr}/moe${model.hp.is_moe}/dubug
    # dir: outputs/${hydra.job.name}/lr${model.hp.lr}/moe${model.hp.is_moe}/${now:%Y-%m-%d_%H-%M} # 多卡时不要用now
    dir: outputs/${hydra.job.name}/lr${model.hp.lr}/moe${model.hp.is_moe}/${now:%Y-%m-%d_%H-%M} 
  
output_dir: ${hydra:run.dir} 
# output_dir: outputs/eval.arp_plus/`date +"%Y-%m-%d_%H-%M"`+'stack_cups_16666_lr1e-4'
wandb: 
  project: arp_moe # before null
  use_wandb: false
  use_proxy: true
  proxy_settings:
    http_proxy: "http://10.81.36.233:7890"
    https_proxy: "http://10.81.36.233:7890"
    wandb_init_timeout: "600"
    wandb_http_timeout: "300"

tcp:
  host: 127.0.0.1
  port: 5050
  max_queue_size: 10
  max_retries: null # retry forever
  retry_interval: 3.0 # seconds
