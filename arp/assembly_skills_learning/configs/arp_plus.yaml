py_module: autoregressive_policy_plus

model:
  weights: null
  # weights: /opt/data/private/arp/arp/assembly_skills_learning/outputs/moe_1GPU/lr1.25e-05/Emb_moeTrue_Transformer_moeFalse/2025_07_14_36BS_10epochs/2025-07-14_16-26/model_30000.pth
  
  hp:
    # rot_z_weight_factor: 1
    # rot_z_weight_max: 2
    is_emb_moe: true
    is_transformer_moe: false # true
    moe_weight: 1 #2 
    moe:
      multiple_gate: True # false
      num_experts: 4 #16 #8
      k: 1 #2 #4 #可以留一个num_moe_layers的接口
      num_shared_experts: 0 # not available yet
    add_corr: true
    add_depth: true
    add_lang: true
    add_pixel_loc: true
    add_proprio: true
    attn_dim: 512
    attn_dim_head: 64
    attn_dropout: 0.1
    attn_heads: 8
    depth: 8
    feat_dim: 220 # 72*3 + 4
    im_channels: 64
    point_augment_noise: 0.05
    img_feat_dim: 3
    img_patch_size: 14
    img_size: 224
    lang_dim: 512
    lang_len: 77
    norm_corr: true
    pe_fix: true
    proprio_dim: 3 # 4 # 18
    mvt_cameras: ['top', 'left', 'front']
    stage2_zoom_scale: 4
    stage2_waypoint_label_noise: 0.05
    rotation_aug: #null
      - [-2, -1, 0, -1, -2]
      - [0.1, 0.2, 0.4, 0.2, 0.1]
    use_xformers: true

    gt_hm_sigma: 1.5
    move_pc_in_bound: true
    place_with_mean: false

    amp: True
    bnb: True

    # lr should be thought on per sample basis
    # effective lr is multiplied by bs * num_devices
    lr: 1.25e-5 # 1e-4 #before 1.25e-5 # 1e-4 # 在train的时候cfg.model.hp.lr *= (cfg.train.num_gpus * cfg.train.bs),少GPU应对应增大lr
    warmup_steps: 2000
    optimizer_type: lamb
    lr_cos_dec: true
    add_rgc_loss: true
    transform_augmentation: true
    transform_augmentation_xyz: [0.125, 0.125, 0.125]
    transform_augmentation_rpy: [0.0, 0.0, 45.0]
    lambda_weight_l2: 1e-4 # 1e-6
    num_rotation_classes: 72

    cos_dec_max_step: -1 # will be override during training

    render_with_cpp: true

    # arp_cfg: 
    #   TODO: true


env:
  tasks: piston_sleeve_installation # stack_cups, before all
  cameras: ["front","left_shoulder", "right_shoulder", "overhead"]
  scene_bounds: [-0.5, -0.5, -0.2, 0.5, 0.5, 0.5] # [-0.3, -0.5, 0.6, 0.7, 0.5, 1.6] # [x_min, y_min, z_min, x_max, y_max, z_max] - the metric volume to be voxelized,待确认
  image_size: 512 #　TODO　有改动要记得修改
  time_in_state: false
  voxel_size: 100
  episode_length: 25 # TODO 不对，encode time 相关
  rotation_resolution: 5
  origin_style_state: true

train:
  bs: 36 #48 #before 96 # 48
  demo_folder: ./data/train
  epochs: 16 # 16 # before 100 # 16
  num_gpus: 1 # 1 # before 4
  num_workers: 4 # before 8, need larger value
  num_transitions_per_epoch: 160000 # 80000 #before 160000 # iters=num_transitions_per_epoch*epochs/bs/world_size
  disp_freq: 100
  cached_dataset_path: null
  save_freq: 2000 # before 10000
  eval_mode: false
  k2k_sample_ratios: 
    piston_sleeve_installation: 1.0
  variation_path:  all_variations # variation0 # all_variations
  episode_path: null # episode4 # null

eval:
  datafolder: ./data/test
  episode_num: 25
  start_episode: 0
  headless: true
  save_video: false
  device: 0


# ================================================ #

hydra:
  job:
    name: moe_${train.num_gpus}GPU #pick_green_var0 # test # default # arp_plus # eval.arp_plus
    chdir: false
  run:
    dir: outputs/${hydra.job.name}/lr${model.hp.lr}/Emb_moe${model.hp.is_emb_moe}_Transformer_moe${model.hp.is_transformer_moe}/${now:%Y_%m_%d}_${train.bs}BS_${train.epochs}epochs/${now:%Y-%m-%d_%H-%M} 
    
output_dir: ${hydra:run.dir} 

wandb: 
  project: arp_moe # before null
  use_wandb: false
  use_proxy: true
  proxy_settings:
    http_proxy: "http://10.81.36.233:7890"
    https_proxy: "http://10.81.36.233:7890"
    wandb_init_timeout: "600"
    wandb_http_timeout: "300"

swanlab:
  project: ATG_MoE
  use_swanlab: true